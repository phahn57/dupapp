---
title: "Pubmed_dupuytren_ver2"
author: "Peter Hahn"
date: "4 11 2018"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message=FALSE, fig.width = 12, fig.height = 8)
## load libraries
library(tidyverse)
library(RISmed)
library(wordcloud)
library(stringr)
library(igraph)
library(ggraph)
library(tidygraph)
library(tm)
library(tidytext)
library(widyr)
library(topicmodels)


```

# Loading data
For this analysis a pubmed query was done with search-term: "Dupuytren Contracture[Mesh]". That reveals 2286 documents. 
From the query this fields were extracted: title, abstract, journal, DOI, year
```{r}
 # load prefabricated csv
        pm_data <- read_csv("abstracts.csv")

# make bins for publication year
        pm_data$year_cut <- cut(pm_data$year, c(1960, 1970, 1980, 1990, 2000,2010,2018),labels=c("60-69","70-79","80-89","90-99","00-09","10-18"), include.lowest=TRUE)
        
        
## remove plural words
       pm_data <-pm_data  %>% mutate(abstract=str_replace_all(abstract,"options","option")) 
# list of words to remove in token
        del_word <-  tibble(word=c("dupuytren","NA","patient","disease","treatment","study","result","hand","disease",
                                "contracture","patients","dupuytren's","clinical","results","report","included","month","treated","found","increased","compared","95","ci","10","20","30","statistically","lt","studies","underwent","diagnosis","12","15","50","reported","de","quot","des","bev","major","dd"))
        

```
# Number of papers per year: self- explaining
```{r include=TRUE}
        pm_data %>%
          group_by(year) %>%
          count() %>%
          #filter(year > 1960) %>%
          ggplot(aes(year, n)) +
          geom_point() +
          geom_line() +
          labs(title = "Pubmed articles with search term `Dupuytren` \n1960-2018", hjust = 0.5, y = "Articles")
```          
## Basic word analysis: 
For text-analysis two steps are necessary:

-unnesting
-removing "stop-words"

The first step builds a dataframe of single words contained within the title or abstract. In the second step common words as: "the, and, or, is.." are removed. Additionally I removed some words and numbers which appear frequently in all abstracts, but make no sense for further analysis:
*word=c("dupuytren","NA","patient","disease","treatment","study","result","hand","disease",
                                "contracture","patients","dupuytren's","clinical","results","report","included","month","treated","found","increased","compared","95","ci","10","20","30","statistically","lt","studies","underwent","diagnosis","12","15","50","reported","de","quot","des","bev","major","dd"))*
                                
```{r}
        pm_title <- pm_data %>% 
        unnest_tokens(word,title) %>% 
        anti_join(stop_words)

        pm_abstract <- pm_data %>% 
                unnest_tokens(word,abstract) %>% 
                anti_join(stop_words)
       
        # remove custom stop_words and remove numbers in abstract
        pm_title <- pm_title %>% 
                anti_join(del_word)
        
        pm_abstract <- pm_abstract %>% 
                anti_join(del_word)  
        pm_abstract <- pm_abstract %>% 
                filter(!word %in% c(0:9))
        
        ## todo replace numbers ??
        
```
# count most frequent words in title
```{r include=TRUE}

        pm_title  %>% count(word, sort=TRUE)
```
# count most frequent words in abstract
```{r include=TRUE}
# count most frequent words in abtsract
        pm_abstract  %>% count(word, sort=TRUE)

```
## pairwise count
Next I am interested in the pairwise appearence of words. Which pair of words appears most frequently in title or abstract. They don't need to be adjacent.

### First for title
```{r include=TRUE}
        title_word_pairs <- pm_title %>% pairwise_count(word,DOI,sort=TRUE,upper= FALSE)
        title_word_pairs
```


### Next for abstract
```{r include=TRUE}
        abstract_word_pairs <- pm_abstract %>% pairwise_count(word,DOI,sort=TRUE, upper=FALSE)
        abstract_word_pairs
```
### Plotting networks of word-pairs
first for title occurence greater 10
```{r include=TRUE}
set.seed(1234)
        title_word_pairs %>%
                filter(n>10) %>% 
                graph_from_data_frame() %>% 
                ggraph(layout="fr")+ 
                geom_edge_link(aes(edge_alpha=n,edge_width=n))+geom_node_point(size=5)+geom_node_text(aes(label=name),repel=TRUE, point.padding=unit(0.2,"lines"))+theme_graph(foreground = 'steelblue', fg_text_colour = 'white')
        

```
next for abstract, occurence greater 50
```{r include=TRUE}
        set.seed(1234)
        abstract_word_pairs %>%
                filter(n>50) %>% 
                graph_from_data_frame() %>% 
                ggraph(layout="kk")+ 
                geom_edge_link(aes(edge_alpha=n,edge_width=n))+geom_node_point(size=3)+geom_node_text(aes(label=name),repel=TRUE, point.padding=unit(0.2,"lines"))+theme_void()
       
```



```{r eval=FALSE}
        abst_tf_idf <- pm_abstract %>% 
  count(DOI, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, DOI, n)

 abst_tf_idf  %>% arrange(-tf_idf)

```





## Topic modelling with LDA
[https://www.tidytextmining.com/topicmodeling.html]:
In text mining, we often have collections of documents, such as blog posts or news articles, that we’d like to divide into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.
Latent Dirichlet allocation is one of the most common algorithms for topic modeling. Without diving into the math behind the model, we can understand it as being guided by two principles.

Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.
LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.

[https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation]

```{r}
        word_counts <- pm_abstract %>% 
        anti_join(stop_words) %>% anti_join(del_word) %>% 
        count(DOI,word,sort=TRUE) %>% ungroup()
```

make document term matrix
```{r}
        abstr_dtm <- word_counts %>% 
        cast_dtm(DOI,word,n)
```



  
```{r}
        abstr_lda <- LDA(abstr_dtm,k=12,control=list(seed=1234))
```

```{r}
        tidy_lda <- tidy(abstr_lda)
```
## top terms

```{r}
        top_terms <- tidy_lda %>% 
        group_by(topic) %>%
        filter(!is.na(term)) %>% 
        top_n(10,beta) %>% 
        ungroup() %>% 
        arrange(topic,-beta)
        
```

### Graphical representation of top terms
12 topics were bulid for the abstracts. After test of 6,8,10,16,24 topics best results are achieved with 12 topics.
For each topic the top 10 terms representing the topic are presented.
```{r include=TRUE}
        top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")

```


# Vice versa representation
We just explored which words are associated with which topics. Next, let’s examine which topics are associated with which description fields (i.e., documents). We will look at a different probability for this, γ, the probability that each document belongs in each topic.

```{r}
lda_gamma <- tidy(abstr_lda,matrix="gamma")
```

## How are the probabilities distributed? 
First notice that the y-axis is plotted on a log scale; otherwise it is difficult to make out any detail in the plot. Next, notice that γ
  runs from 0 to 1; remember that this is the probability that a given document belongs in a given topic. There are many values near zero, which means there are many documents that do not belong in each topic. Also, there are many values near γ=1
 ; these are the documents that do belong in those topics. This distribution shows that documents are being well discriminated as belonging to a topic or not. 

```{r}
        ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```

We can also look at how the probabilities are distributed within each topic.
```{r}
        ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```

