---
title: "julia_stm"
author: "Peter Hahn"
date: "11 11 2018"
output: html_document
---
#Links
[https://github.com/dondealban/learning-stm#workflow]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message=FALSE, fig.width = 12, fig.height = 8)
## load libraries
library(tidyverse)
#library(RISmed)
library(wordcloud)
library(stringr)
library(igraph)
#library(ggraph)
#library(tidygraph)
library(tm)
library(tidytext)
#library(widyr)
library(topicmodels)
library(furrr)
library(stm)
```
# Loading data alternatively laod prepared files
For this analysis a pubmed query was done with search-term: "Dupuytren Contracture[Mesh]". That reveals 2286 documents. 
From the query this fields were extracted: title, abstract, journal, DOI, year
```{r}
 # load prefabricated csv build by first part of preprocess.Rmd
        pm_data <- read_csv("abstracts.csv")

# make bins for publication year
        pm_data$year_cut <- cut(pm_data$year, c(1960, 1970, 1980, 1990, 2000,2010,2018),labels=c("60-69","70-79","80-89","90-99","00-09","10-18"), include.lowest=TRUE)
        
        
## remove plural words
       pm_data <-pm_data  %>% mutate(abstract=str_replace_all(abstract,"options","option")) 
# list of words to remove in token
        del_word <-  tibble(word=c("dupuytren","NA","patient","disease","treatment","study","result","hand","disease",
                                "contracture","patients","dupuytren's","clinical","results","report","included","month","treated","found","increased","compared","95","ci","10","20","30","statistically","lt","studies","underwent","diagnosis","12","15","50","reported","de","quot","des","bev","major","dd"))
        

```


# Number of papers per year: self- explaining
```{r include=TRUE}
        pm_data %>%
          group_by(year) %>%
          count() %>%
          #filter(year > 1960) %>%
          ggplot(aes(year, n)) +
          geom_point() +
          geom_line() +
          labs(title = "Pubmed articles with search term `Dupuytren` \n1960-2018", hjust = 0.5, y = "Articles")
```          

## Basic calc

```{r}
        pm_abstract <- pm_data %>% 
                unnest_tokens(word,abstract) %>% 
                anti_join(stop_words)
        ## some special filters
        pm_abstract <- pm_abstract %>% 
                anti_join(del_word)  
        pm_abstract <- pm_abstract %>% 
                filter(!word %in% c(0:9))
```

```{r}
        word_counts <- pm_abstract %>% 
        count(DOI,word,sort=TRUE) %>% ungroup()
```

## make document term matrix two ways
```{r}
        abstr_dtm <- word_counts %>% 
        cast_dtm(DOI,word,n)

        abstr_sparse <- word_counts %>% 
                cast_sparse(DOI,word,n)
```

### calculate stm better use lda

```{r}
plan(multiprocess)

many_models <- data_frame(K = c(40,50,60,70,90,110)) %>%
  mutate(topic_model = future_map(K, ~stm(abstr_sparse, K = .,
                                          verbose = FALSE)))
```

## evaluate

```{r}
        heldout <- make.heldout(abstr_sparse)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, abstr_sparse),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, abstr_sparse),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

k_result
```

# plot

```{r}
        k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 24")
```

## final model with k = 24

```{r}
topic_model <- stm(abstr_sparse, K = 30, 
                   verbose = FALSE, init.type = "Spectral")


```
 
## alternative with LD
```{r}
        abstr_lda <- LDA(abstr_dtm,k=30,control=list(seed=1234))
```


```{r}
        ##save if necessary or load
        # save(topic_model,file="topic_60.RData")
        tidy_lda <- tidy(abstr_lda)
```
## top terms

```{r}
        top_terms <- tidy_lda %>% 
        group_by(topic) %>%
        filter(!is.na(term)) %>% 
        top_n(10,beta) %>% 
        ungroup() %>% 
        arrange(topic,-beta)
        
```

### Graphical representation of top terms
12 topics were bulid for the abstracts. After test of 6,8,10,16,24 topics best results are achieved with 12 topics.
For each topic the top 10 terms representing the topic are presented.
```{r include=TRUE}
        top_terms %>% 
  mutate(term = reorder(term, beta)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  mutate(term = factor(paste(term, topic, sep = "__"), 
                       levels = rev(paste(term, topic, sep = "__")))) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 6, scales = "free")

```

# Vice versa representation
We just explored which words are associated with which topics. Next, let’s examine which topics are associated with which description fields (i.e., documents). We will look at a different probability for this, γ, the probability that each document belongs in each topic.

```{r}
lda_gamma <- tidy(abstr_lda,matrix="gamma")
```

## How are the probabilities distributed? 
First notice that the y-axis is plotted on a log scale; otherwise it is difficult to make out any detail in the plot. Next, notice that γ
  runs from 0 to 1; remember that this is the probability that a given document belongs in a given topic. There are many values near zero, which means there are many documents that do not belong in each topic. Also, there are many values near γ=1
 ; these are the documents that do belong in those topics. This distribution shows that documents are being well discriminated as belonging to a topic or not. 

```{r}
        ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```

We can also look at how the probabilities are distributed within each topic.
```{r}
        ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```





## prepare mesh
```{r}
        MedList = mapply(cbind, "ID"=ArticleId(fetch),Mesh(fetch),SIMPLIFY = FALSE)
        m_list <-Mesh(fetch) 
        m_list_db <- bind_rows(mesh_head)
        mesh_melt <-melt(MedList,id="ID")
        mesh_melt <- mesh_melt[,3:5]
        mesh_melt <- mesh_melt %>% filter(variable=="Heading")
                ##from Jenny Bryan purrr tutorial
                #mesh_head <- map(MedList,2)
                ## inspect
                #listviewer::jsonedit(mesh_head)
                
        
```

## gamma
```{r}
        lda_gamma <- tidy(topic_model,matrix="gamma",document_names = abstr_sparse[])
## join with abstracts   

        joined <- left_join(gamma,abstracts,by=c("document"="DOI"))

        top_gamma <- lda_gamma %>% group_by(document) %>% arrange(desc(gamma)) %>% top_n(2,gamma) %>% arrange(desc(document))
        
# plot topic by year
        joined %>% group_by(year,topic) %>% count() %>% 
                ggplot(aes(year,n))+geom_point()+facet_wrap(~topic)
        
```

### join papers and gamma values

```{r}
yy <- abstracts %>% filter(DOI==10050243)  ## number == document in topic_model
```

